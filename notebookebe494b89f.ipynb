{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90827,"databundleVersionId":10610319,"sourceType":"competition"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# Path to the dataset folder\ndataset_dir = '/kaggle/input/datathon-ai-confluence-iitg-24'\n\n# List all directories and files inside the dataset folder\nprint(\"Contents of the dataset folder:\", os.listdir(dataset_dir))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:05:40.559615Z","iopub.execute_input":"2024-12-27T12:05:40.560003Z","iopub.status.idle":"2024-12-27T12:05:40.566290Z","shell.execute_reply.started":"2024-12-27T12:05:40.559963Z","shell.execute_reply":"2024-12-27T12:05:40.565077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Path to the dataset_public folder\ndataset_public_dir = '/kaggle/input/datathon-ai-confluence-iitg-24/Dataset'\n\n# List all directories and files inside the dataset_public folder\nprint(\"Contents of the dataset_public folder:\", os.listdir(dataset_public_dir))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:05:40.567437Z","iopub.execute_input":"2024-12-27T12:05:40.567842Z","iopub.status.idle":"2024-12-27T12:05:40.585141Z","shell.execute_reply.started":"2024-12-27T12:05:40.567809Z","shell.execute_reply":"2024-12-27T12:05:40.584317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Path to the train directory\ntrain_dir = '/kaggle/input/datathon-ai-confluence-iitg-24/Dataset/train'\n\n# List all categories in the train directory\ncategories = os.listdir(train_dir)\n\nprint(\"Available categories:\", categories)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:05:40.585962Z","iopub.execute_input":"2024-12-27T12:05:40.586238Z","iopub.status.idle":"2024-12-27T12:05:40.601997Z","shell.execute_reply.started":"2024-12-27T12:05:40.586213Z","shell.execute_reply":"2024-12-27T12:05:40.600968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\n# Define image size (you can adjust it based on your requirements)\nimage_size = (64, 64)\n\n# Initialize lists to store the images and labels\nX = []\ny = []\n\n# Iterate through each category and load images\nfor category in categories:\n    category_path = os.path.join(train_dir, category)  # Get path to category folder\n    for img_name in os.listdir(category_path):  # Iterate through each image\n        img_path = os.path.join(category_path, img_name)\n        \n        # Read and resize the image\n        img = cv2.imread(img_path)\n        img = cv2.resize(img, image_size)  # Resize to uniform size\n        \n        # Append the image and corresponding label\n        X.append(img)\n        y.append(category)\n\n# Convert lists to numpy arrays\nX = np.array(X)\ny = np.array(y)\n\nprint(f\"Loaded {len(X)} images from {len(categories)} categories.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:05:40.602868Z","iopub.execute_input":"2024-12-27T12:05:40.603125Z","iopub.status.idle":"2024-12-27T12:06:18.814930Z","shell.execute_reply.started":"2024-12-27T12:05:40.603103Z","shell.execute_reply":"2024-12-27T12:06:18.813940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\n# Step 1: Label Encoding the categories\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Step 2: One-hot encoding the labels\ny_one_hot = to_categorical(y_encoded)\n\nprint(f\"Encoded labels: {y_encoded[:5]}\")  # Show the first 5 encoded labels\nprint(f\"One-hot encoded labels: {y_one_hot[:5]}\")  # Show the first 5 one-hot encoded labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:18.816134Z","iopub.execute_input":"2024-12-27T12:06:18.816486Z","iopub.status.idle":"2024-12-27T12:06:18.823870Z","shell.execute_reply.started":"2024-12-27T12:06:18.816447Z","shell.execute_reply":"2024-12-27T12:06:18.822912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize the images\nX_normalized = X / 255.0\n\nprint(f\"Image shape after normalization: {X_normalized.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:18.824982Z","iopub.execute_input":"2024-12-27T12:06:18.825363Z","iopub.status.idle":"2024-12-27T12:06:18.953798Z","shell.execute_reply.started":"2024-12-27T12:06:18.825327Z","shell.execute_reply":"2024-12-27T12:06:18.952827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_normalized, y_one_hot, test_size=0.2, random_state=42)\n\nprint(f\"Training set size: {X_train.shape[0]}, Validation set size: {X_val.shape[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:18.954772Z","iopub.execute_input":"2024-12-27T12:06:18.955025Z","iopub.status.idle":"2024-12-27T12:06:19.070586Z","shell.execute_reply.started":"2024-12-27T12:06:18.955003Z","shell.execute_reply":"2024-12-27T12:06:19.069812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n# Build the CNN model\nmodel = Sequential()\n\n# Add layers to the model\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))  # Convolutional layer\nmodel.add(MaxPooling2D(pool_size=(2, 2)))  # Pooling layer\nmodel.add(Conv2D(64, (3, 3), activation='relu'))  # Another Convolutional layer\nmodel.add(MaxPooling2D(pool_size=(2, 2)))  # Pooling layer\nmodel.add(Flatten())  # Flatten the output\nmodel.add(Dense(128, activation='relu'))  # Fully connected layer\nmodel.add(Dropout(0.5))  # Dropout to prevent overfitting\nmodel.add(Dense(len(categories), activation='softmax'))  # Output layer with softmax activation\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Summary of the model\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:19.071627Z","iopub.execute_input":"2024-12-27T12:06:19.071979Z","iopub.status.idle":"2024-12-27T12:06:19.171997Z","shell.execute_reply.started":"2024-12-27T12:06:19.071953Z","shell.execute_reply":"2024-12-27T12:06:19.170907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n\n# Save the model after training\nmodel.save('vehicle_classification_model.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:19.173289Z","iopub.execute_input":"2024-12-27T12:06:19.173661Z","iopub.status.idle":"2024-12-27T12:06:29.859917Z","shell.execute_reply.started":"2024-12-27T12:06:19.173621Z","shell.execute_reply":"2024-12-27T12:06:29.858917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model on the validation set\nval_loss, val_accuracy = model.evaluate(X_val, y_val)\nprint(f\"Validation Loss: {val_loss}\")\nprint(f\"Validation Accuracy: {val_accuracy}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:29.860888Z","iopub.execute_input":"2024-12-27T12:06:29.861232Z","iopub.status.idle":"2024-12-27T12:06:30.106566Z","shell.execute_reply.started":"2024-12-27T12:06:29.861198Z","shell.execute_reply":"2024-12-27T12:06:30.105801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Val'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Val'], loc='upper left')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:30.110526Z","iopub.execute_input":"2024-12-27T12:06:30.110836Z","iopub.status.idle":"2024-12-27T12:06:30.644141Z","shell.execute_reply.started":"2024-12-27T12:06:30.110809Z","shell.execute_reply":"2024-12-27T12:06:30.643213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load new image\nfrom keras.preprocessing import image\nimg = image.load_img('/kaggle/input/datathon-ai-confluence-iitg-24/Dataset/train/Cars/1.jpg', target_size=(64, 64))  # Adjust size as needed\nimg_array = image.img_to_array(img)  # Convert to numpy array\nimg_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\nimg_array = img_array / 255.0  # Normalize the image\n\n# Predict\nprediction = model.predict(img_array)\npredicted_class = label_encoder.inverse_transform([np.argmax(prediction)])\nprint(f\"Predicted class: {predicted_class}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:30.645755Z","iopub.execute_input":"2024-12-27T12:06:30.646164Z","iopub.status.idle":"2024-12-27T12:06:30.872165Z","shell.execute_reply.started":"2024-12-27T12:06:30.646124Z","shell.execute_reply":"2024-12-27T12:06:30.871325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set a confidence threshold\nconfidence_threshold = 0.7\n\n# Get the class with the highest probability\nprediction = model.predict(img_array)\npredicted_class_prob = np.max(prediction)\npredicted_class = np.argmax(prediction)\n\n# Check if the confidence is above the threshold\nif predicted_class_prob >= confidence_threshold:\n    predicted_label = label_encoder.inverse_transform([predicted_class])\n    print(f\"Predicted class: {predicted_label} with confidence: {predicted_class_prob:.2f}\")\nelse:\n    print(f\"Prediction confidence is too low: {predicted_class_prob:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:30.873084Z","iopub.execute_input":"2024-12-27T12:06:30.873434Z","iopub.status.idle":"2024-12-27T12:06:30.946890Z","shell.execute_reply.started":"2024-12-27T12:06:30.873399Z","shell.execute_reply":"2024-12-27T12:06:30.945946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# List of class labels used during training\nclass_labels = ['Cars', 'Truck', 'Auto Rickshaws', 'Bus', 'Bicycles', 'Trains', 'Motorcycles']  # Replace with your actual class labels\n\n# Recreate the LabelEncoder\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(class_labels)\n\n# Save the recreated LabelEncoder for future use\nimport pickle\nwith open('label_encoder.pkl', 'wb') as f:\n    pickle.dump(label_encoder, f)\nprint(\"LabelEncoder recreated and saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:30.947889Z","iopub.execute_input":"2024-12-27T12:06:30.948167Z","iopub.status.idle":"2024-12-27T12:06:30.954594Z","shell.execute_reply.started":"2024-12-27T12:06:30.948144Z","shell.execute_reply":"2024-12-27T12:06:30.953770Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.models import load_model\nfrom keras.preprocessing import image\nimport numpy as np\nimport pandas as pd\nimport os\n\n# Load the trained model\nmodel = load_model('vehicle_classification_model.h5')\n\n# Load the label encoder (if you have it saved as a file, replace with its path)\nimport pickle\nwith open('label_encoder.pkl', 'rb') as f:  # Adjust the path if necessary\n    label_encoder = pickle.load(f)\n\n# Define confidence threshold\nconfidence_threshold = 0.7","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:30.955643Z","iopub.execute_input":"2024-12-27T12:06:30.956253Z","iopub.status.idle":"2024-12-27T12:06:31.052430Z","shell.execute_reply.started":"2024-12-27T12:06:30.956205Z","shell.execute_reply":"2024-12-27T12:06:31.051474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom keras.preprocessing import image\nimport numpy as np\nfrom keras.models import load_model\nfrom PIL import ImageFile, Image\n\n# Allow loading of truncated images\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n# Paths\ntest_dir = '/kaggle/input/datathon-ai-confluence-iitg-24/Dataset/test'  # Update to your test dataset folder\nmodel_path = 'vehicle_classification_model.h5'\nmodel = load_model(model_path)\n\n# Label encoder (recreate or load as needed)\nfrom sklearn.preprocessing import LabelEncoder\nlabels = ['Cars', 'Trucks', 'Buses', 'Motorcycles', 'Trains', 'Bicycles', 'Auto Rickshaws']\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(labels)\n\n# Validate images to exclude corrupted files\nvalid_images = []\ntest_images = sorted(os.listdir(test_dir))\nfor img_name in test_images:\n    img_path = os.path.join(test_dir, img_name)\n    try:\n        with Image.open(img_path) as img:\n            img.verify()  # Verify image integrity\n            valid_images.append(img_name)\n    except Exception as e:\n        print(f\"Skipping corrupted image: {img_name} ({e})\")\n\n# Initialize results\nresults = []\n\nfor img_name in valid_images:\n    img_path = os.path.join(test_dir, img_name)\n    try:\n        # Load and preprocess image\n        img = image.load_img(img_path, target_size=(64, 64))  # Match training image size\n        img_array = image.img_to_array(img)\n        img_array = np.expand_dims(img_array, axis=0)\n        img_array = img_array / 255.0\n\n        # Predict\n        prediction = model.predict(img_array)\n        predicted_class = np.argmax(prediction)\n        predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n\n        # Remove the file extension\n        image_id = os.path.splitext(img_name)[0]\n\n        # Append result\n        results.append({'ID': image_id, 'class': predicted_label})\n    except Exception as e:\n        print(f\"Error processing {img_name}: {e}\")\n\n# Create DataFrame\nsubmission_df = pd.DataFrame(results)\n\n# Save as CSV\nsubmission_file = 'submission.csv'\nsubmission_df.to_csv(submission_file, index=False)\nprint(f\"Submission saved as {submission_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:06:31.053404Z","iopub.execute_input":"2024-12-27T12:06:31.053742Z","iopub.status.idle":"2024-12-27T12:07:52.923994Z","shell.execute_reply.started":"2024-12-27T12:06:31.053692Z","shell.execute_reply":"2024-12-27T12:07:52.922847Z"}},"outputs":[],"execution_count":null}]}